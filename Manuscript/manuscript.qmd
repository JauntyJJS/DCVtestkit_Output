---
title: "*lancer*: an R package for linearity assessment and visualisation of multiple curves"

bibliography: bibliography.bib
csl: bioinfo.csl
format: docx

crossref:
  fig-title: Fig.
  fig-prefix: Fig.
  subref-labels: alpha A
  title-delim: .
---

```{r R packages}
#| echo: false
#| message: false

library(dplyr)
library(stringr)
library(here)
library(labelled)
library(gtsummary)
library(flextable)
library(webshot2)

```

## Manuscript Type

Application Note

# Abstract

## Summary

Linearity assessment plays a significant role in the validation of instrumentation and experimental procedures. Linearity can be tested by applying several graphical and numerical approaches. Commonly used spreadsheet software, e.g. Microsoft Excel, only allow the analyst to repetitively plot, view and analyse the linearity of curves one at a time, a tedious and time-consuming process. In addition, summary statistics of these curves are mostly limited to the Pearson Correlation Coefficient, which is insufficient to fully test the for linearity. While plotting of many curves at once and calculation of additional summary statistics for assessing linearity of curves can be done using programming languages like R, implementing this from scratch can be arduous task for the analyst. As such, we created the R package *lancer* so that curves can be analysed efficiently with a few functions. To our knowledge, this is currently not implemented in other software or programming languages. *lancer* furthermore reports the statistical results in tables/spreadsheets and records the plots in a pdf file. In addition, *lancer* can also create interactive trellis plots, displayed as a HTML folder, for exploratory analysis, helping analysts when dealing with large datasets.

## Availability and implementation

*lancer* is available on GitHub <https://github.com/SLINGhub/lancer>. The documentation and tutorials can be accessed from <https://slinghub.github.io/lancer/>

## Supplementary information

Supplementary data are available at *Bioinformatics* online.

## Issue Section

Data and text mining

{{< pagebreak >}}

# Introduction

Linearity assessment is an important performance test of an analytical instrument or method when a linear response is assumed. It is applied in many fields in analytical sciences, such as assay development (@ross2003 and @hsieh2008), calibration/dilution studies (@rodríguez1993 and @sands2021) and laboratory tests. After analysis, curves representing response values versus concentration are plotted for each analyte of interest. As the accuracy of an analytical method is linked to linearity, a non-linear behaviour must be recognized and addressed accordingly. Visual inspection of curves is still useful but it must be accompanied by statistical tests as decision parameters.

To individually plot numerous curves and check for linearity is time-consuming. Furthermore, @sonnergaard2006 suggests that the Pearson Correlation Coefficient, a commonly used test, is an ineffective standalone numeric parameter to estimate linearity. While researchers like @vanloco2002, @sanchez2021 and @logue2018 have indicated other metrics for linearity evaluation, these metrics are generally not implemented in most software.

The R package *lancer* addresses these issues by assisting analysts to plot curves from many experiments, with additional metrics that better describe the characteristics of the curve. It also provides users with an interactive viewer to group, filter and sort the curves, allowing the examination of problematic cases, such as the ones generated by saturated signals.

# Approach

Using response curves in metabolomic/lipidomic studies from @croixmarie2009 and @sands2021 as an example, Supplementary Figure 1 depicts the workflow of *lancer*. The workflow starts with two tables:

*    Curve Batch Annotation, describing the curve(s), such as concentration (x-axis) and response curve batches, if any.
*    Curve Signal Data, containing response values (y-axis) for each sample and curve. 

Using a common column Sample Name, the two tables can be merged into one table (Curve Table) via `create_curve_table`.

Next, summary statistics are calculated via `summarise_curve_table` for each curve. Besides the Pearson Correlation Coefficient, an additional statistical test is the Mandel’s Fitting Test ($F_{stats}$ in @eq-mandel-test) as described by @andrade2013. The $F$ test can give evidence that a quadratic model fits better than a linear model, indicating a non-linear curve.

$$
\begin{aligned}
&F_{stats} = \frac{(n-2) \ \mathrm S_{lin}^2 \ - \ (n-3) \ \mathrm S_{quad}^2}{\mathrm S_{quad}^2} \sim \ F(1,n-3)
\\
\\
&\text{where}
\\
&\mathrm S_{lin}^2 = \frac{1}{n-2} \sum_{i=1}^{n} ( y_{i,lin} - y_{i,true})^2 
\\
&\mathrm S_{quad}^2 = \frac{1}{n-3} \sum_{i=1}^{n} ( y_{i,quad} - y_{i,true})^2
\\ 
&y_{i,lin} \ \text{is the linear model predicted y-axis value.}
\\ 
&y_{i,quad} \ \text{is the quadratic model predicted y-axis value.}
\\ 
&y_{i,true} \ \text{is the true y-axis value.}
\\ 
&n \ \text{is the number of data points.}
\end{aligned}
$$ {#eq-mandel-test} Another statistical test is Percent Residual Accuracy ($\%RA$ in @eq-pra) from @logue2018. Ranging from $-\infty$ to $100$, according to which a perfectly linear curve gives a $\%RA$ value of $100$.

$$
\begin{aligned}
&\%RA = \frac{100\%}{n} \sum_{i=1}^{n}\left(1 - \bigg| \frac{x_{i,true}-x_{i,lin}}{x_{i,true}} \bigg|\right)
\\
\\
&\text{where}
\\
&x_{i,true} \ \text{is the true x-axis value.}
\\ 
&x_{i,lin} \ \text{is the linear model predicted x-axis value.}
\\ 
&n \ \text{is the number of data points.}
\end{aligned}
$$ {#eq-pra}

The software also calculates the concavity of the fitted quadratic model, to identify if the curve is dominantly non-linear at high (concavity $<0$) or low (concavity $>0$) concentrations.

Supplementary Figure 2 gives the summary statistics of three simulated curves (as examples), where one curve is linear and two curves show a plateau at high concentrations (denoted as saturation regime curve) and low concentrations (denoted as noise regime curve), respectively. The corresponding Pearson Correlation Coefficient values (`r_corr`) are $\ge0.9$ ($0.99$, $0.95$ and $0.98$ respectively) even for the non-linear curves. However, non-linear curves are detected by much lower Mandel’s Fitting Test $p$ values (`mandel_p_val`) ($1.66 * 10^{-4}$ and $2.56 * 10^{-3}$ respectively vs $0.38$ for the linear curve) and Percent Residual Accuracy values (`pra_linear`) ($62.30$ and $74.69$ respectively vs $94.32$ for the linear curve).

{{< pagebreak >}}

::: {#fig-overview layout="[[60,-40], [65,-35]]"}
![Curve Grouping Workflow](images/README-LinearEvaluation.png){#fig-linearity}

![Interactive Visualisation](images/README-TrellisOutput.png){#fig-visualisation}

*lancer*'s curve grouping workflow in @fig-linearity and interactive visualisation of curves in @fig-visualisation.
:::

{{< pagebreak >}}

After calculating the summary statistics for each curve, *lancer* uses the function `evaluate_linearity` to group the curves according to the workflows proposed in @fig-linearity. Workflow 1 uses the Pearson Correlation Coefficient and Percent Residual Accuracy to determine if the curve is linear (labelled as Good Linearity) or not (labelled as Poor Linearity). Workflow 2 goes one step further, using the Mandel’s Fitting Test and the fitted quadratic model’s concavity to check if a non-linear curve plateaus at low (labeled as Noise Regime) or high (labelled as Saturation Regime) concentrations. Non-linear curves that do not follow these trends are labelled as Poor Linearity.

```{r Load Simulated Data Results}
#| echo: false
saturation_regime_curve_summary <-
  readRDS(file = here::here(
    "Quarto_Simulation",
    "saturation_regime_curve_summary.rds")
)

noise_regime_curve_summary <-
  readRDS(file = here::here(
    "Quarto_Simulation",
    "noise_regime_curve_summary.rds")
)

linear_curve_summary <-
  readRDS(file = here::here(
    "Quarto_Simulation",
    "linear_curve_summary.rds")
)
```

```{r Combine Simulated Data Results}
#| echo: false
saturation_regime_curve_summary <- saturation_regime_curve_summary |> 
  dplyr::mutate(curve_group = "Saturation Regime")

noise_regime_curve_summary <- noise_regime_curve_summary |> 
  dplyr::mutate(curve_group = "Noise Regime")

linear_curve_summary <- linear_curve_summary |> 
  dplyr::mutate(curve_group = "Good Linearity")

combined_curve_summary <-
  dplyr::bind_rows(
    saturation_regime_curve_summary,
    noise_regime_curve_summary,
    linear_curve_summary
  ) 
```

```{r Data Wrangling for Simulation Results}
#| echo: false
combined_curve_summary <- 
  combined_curve_summary |> 
  dplyr::mutate(
    curve_group = factor(.data$curve_group,
                         levels = c("Good Linearity",
                                    "Saturation Regime",
                                    "Noise Regime")),
    pearson_group = dplyr::case_when(
      .data$r_corr >= 0.8 ~ "more than or\nequal to 0.8",
      .data$r_corr < 0.8 ~ "less than 0.8",
    ),
    wf2_group = factor(.data$wf2_group,
                       levels = c("Good Linearity",
                                  "Saturation Regime",
                                  "Noise Regime",
                                  "Poor Linearity"))
  ) |> 
  labelled::set_variable_labels(
    ID = "Unique Curve Identifier",
    curve_group = "Simulated Curve Group",
    wf1_group = "Workflow 1 Grouping",
    wf2_group = "Workflow 2 Grouping",
    pearson_group = "Pearson Correlation\nCoefficient Grouping",
    r_corr = "Pearson Correlation\nCoefficient",
    pra_linear = "Percent Residual\nAccuracy",
    mandel_p_val = "Mandel's Test p Value",
    concavity = "Concavity Of Fitted Quadratic Model",
    r2_linear = "Coefficient Of Determination",
    r2_adj_linear = "Adjusted Coefficient Of Determination",
    mandel_stats = "Test statistics from Mandel's Test"
  )

```

```{r Simulation Results Table}
#| echo: false
#| output: false
simulation_results <- combined_curve_summary |> 
  dplyr::select(c("curve_group", "wf2_group", 
                  "pearson_group"
                  # "r_corr", "pra_linear"
                  )
                ) |> 
  dplyr::relocate(c("pearson_group")) |> 
  gtsummary::tbl_summary(
    by = "curve_group",
    statistic = list(
      pearson_group ~ "{n}/{N} ({p}%)",
      wf2_group ~ "{n}/{N} ({p}%)"
    ),
    digits = list(
      wf2_group ~ c(0, 0, 1),
      pearson_group ~ c(0, 0, 1)
      #r_corr ~ c(2),
      #pra_linear ~ c(0)
      )
  ) |> 
  gtsummary::modify_header(
    gtsummary::all_stat_cols() ~ "**{level}**\nN = {n}"
    ) |> 
  gtsummary::modify_spanning_header(
    gtsummary::all_stat_cols() ~ "**Simulated Curve Type**"
    )
```

A benchmark workflow using only Pearson Correlation Coefficient value of $0.8$ is compared with Workflow 2 on simulated data sets of `r nrow(linear_curve_summary)` linear curves (labelled as Linear), curves that plateau at low (labelled as Noise Regime) or high (labelled as Saturation Regime) concentrations. Supplementary Figure 3 shows that Workflow 2 better identifies the saturation and noise regime curves than the benchmark workflow. While Workflow 2 correctly classifies a lower number of linear curves than the benchmark workflow, its percentage of correctly classified linear curves, `r inline_text(simulation_results, variable = wf2_group, level = "Good Linearity", column = "Good Linearity")`, is high. See <https://lancer-simulation.netlify.app> for report details. While the threshold values of Pearson Correlation Coefficient and Percent Residual Accuracy are based on the interpretation of @y.h.chan2003 and @logue2018, they remain subjective and arbitrary. Nevertheless, *lancer* allows optimization of these threshold values according to the analyst’s preference.

Although *lancer* can export the results in Excel or pdf, an interactive user interface can generate a better overview of the data. @fig-visualisation shows a HTML folder, exported by *lancer*, such that clicking on the index.html file inside the folder will open an interactive trellis plot that can group, filter and sort curves. This allows room for exploratory data analysis, such as identifying curves with linearity issues or understanding the effects of changing the Pearson Correlation Coefficient threshold to another value. Such information is hard to achieve with other common software. An example of an interactive viewer created by *lancer* can be found at <https://lancer-interactive-example.netlify.app>

# Conclusion

Linearity is one of the most important parameters of an analytical method to be evaluated. Our R package, *lancer*, can estimate linearity efficiently in high-throughput settings and with functions that plot many curves quickly, reporting curve summary statistics to better describe the shape of the datasets. It also provides an interactive trellis plot for exploratory data analysis. It is available on GitHub <https://github.com/SLINGhub/lancer> while the documentation and tutorials can be accessed from <https://slinghub.github.io/lancer>.

# Acknowledgements

These should be included at the end of the text and not in footnotes. Please ensure you acknowledge all sources of funding, see funding section below.

Details of all funding sources for the work in question should be given in a separate section entitled 'Funding'. This should appear before the 'Acknowledgements' section.

# Funding

The following rules should be followed:

-   The sentence should begin: 'This work was supported by ...' -
-   The full official funding agency name should be given, i.e. 'National Institutes of Health', not 'NIH' (full RIN-approved list of UK funding agencies)
-   Grant numbers should be given in brackets as follows: '\[grant number xxxx\]'
-   Multiple grant numbers should be separated by a comma as follows: '\[grant numbers xxxx, yyyy\]'
-   Agencies should be separated by a semi-colon (plus 'and' before the last funding agency)
-   Where individuals need to be specified for certain sources of funding the following text should be added after the relevant agency or grant number 'to \[author initials\]'.

An example is given here: 'This work was supported by the National Institutes of Health \[AA123456 to C.S., BB765432 to M.H.\]; and the Alcohol & Education Research Council \[hfygr667789\].'

Oxford Journals will deposit all NIH-funded articles in PubMed Central. See Depositing articles in repositories -- information for authors for details. Authors must ensure that manuscripts are clearly indicated as NIH-funded using the guidelines above.

# References
