---
title: "*lancer*: an R package for linearity assessment and visualisation of multiple curves"

bibliography: bibliography.bib
csl: bioinfo.csl
format: docx

crossref:
  fig-title: Fig.
  fig-prefix: Fig.
  subref-labels: alpha A
  title-delim: .
---

```{r R packages}
#| echo: false
#| message: false

library(dplyr)
library(stringr)
library(here)
library(labelled)
library(gtsummary)
library(flextable)
library(webshot2)

```

## Manuscript Type

Application Note

# Abstract

## Summary

Linearity assessment plays a significant role in the validation of instrumentation and experimental procedures. As technology progresses, methods are becoming more high throughput, providing analysts with many measurements generated in a short time. Commonly used software, like Excel, only allow the analyst to repetitively plot, view and analyse the linearity of curves one at a time, a tedious and time-consuming process. In addition, summary statistics of these curves are mostly limited to the Pearson Correlation Coefficient which is insufficient to fully clarify the shape of the curves. *lancer* aims to provide additional summary statistics for assessing linearity of curves, taken from previous publications but which are not implemented in the current software tools. It also helps to reduce the analyst's workload by analysing many curves automatically, reporting the statistical results in Excel and recording the plots in a pdf file. In addition, it can also create an interactive trellis displayed as a HTML folder for more exploratory analysis.

## Availability and implementation

*lancer* is available on GitHub <https://github.com/SLINGhub/lancer>. The documentation and tutorials can be accessed from <https://slinghub.github.io/lancer/>

## Supplementary information

Supplementary data are available at *Bioinformatics* online.

## Issue Section

Data and text mining

{{< pagebreak >}}

# Introduction

Linearity assessment is an important quality control to verify if the quantitative acquisition range of an instrument or experimental method is reliable. It is applied in many fields in analytical science such as calibration/dilution studies (@rodr√≠guez1993 and @sands2021) and assay development (@ross2003 and @hsieh2008). After analysis, curves are plotted individually for each analyte of interest with a Pearson Correlation Coefficient value for each plotted curve using general-purpose software like Excel. A non-linear curve suggests possible human, systematic or analytical errors that need to be addressed. Examples of these errors are incorrect amount of analyte, imprecise pipette calibration, expired reagents or equipment malfunction.

However, as instrument or experimental method are becoming more high throughput, many measurements can be done in a short time. Having the analyst to individually plot numerous curves to check for linearity is time-consuming. Furthermore, @sonnergaard2006 warns that the Pearson Correlation Coefficient is an ineffective standalone numeric parameter to estimate linearity. While researchers have created other metrics for linearity evaluation, these metrics are rarely implemented in most general-purpose software.

The R package, *lancer* addresses these issues by assisting analysts, to plot curves from many experiments easily with additional metrics, other than the Pearson Correlation Coefficient, that better describe the curve's shape. It also provides an interactive viewer for analysts to group, filter and sort the curves, allowing them to examine problematic ones, such as saturated curves.

# Approach

Using response curves in metabolomic/lipidomic studies as an example, Supplementary Figure 1 depicts the workflow of *lancer*. The workflow starts with two tables: 

*    Curve Signal Data, containing response signals (y-axis for response curve) for each sample and curve. 
*    Curve Batch Annotation, containing curve related information, such as concentration (x-axis for response curve) and response curve batches if any. 

Using a common column Sample Name, the two tables can be merged into one table (Curve Table) via `create_curve_table`.

Next, summary statistics are calculated via `summarise_curve_table` for each curve. Besides the Pearson Correlation Coefficient, one additional calculation is the Mandel's Fitting Test ($F_{stats}$ in @eq-mandel-test) described by @andrade2013. A low $p$ value from the $F$ test gives sufficient evidence that a quadratic model fits better than a linear model, indicating a non-linear curve.

$$
\begin{aligned}
&F_{stats} = \frac{(n-2) \ \mathrm S_{lin}^2 \ - \ (n-3) \ \mathrm S_{quad}^2}{\mathrm S_{quad}^2} \sim \ F(1,n-3)
\\
\\
&\text{where}
\\
&\mathrm S_{lin}^2 = \frac{1}{n-2} \sum_{i=1}^{n} ( y_{i,lin} - y_{i,true})^2 
\\
&\mathrm S_{quad}^2 = \frac{1}{n-3} \sum_{i=1}^{n} ( y_{i,quad} - y_{i,true})^2
\\ 
&y_{i,lin} \ \text{is the linear model predicted y-axis value.}
\\ 
&y_{i,quad} \ \text{is the quadratic model predicted y-axis value.}
\\ 
&y_{i,true} \ \text{is the true y-axis value.}
\\ 
&n \ \text{is the number of data points.}
\end{aligned}
$$ {#eq-mandel-test} Another calculation is Percent Residual Accuracy ($\%RA$ in @eq-pra) from @logue2018. Ranging from $-\infty$ to $100$, a linear curve gives a $\%RA$ value close to $100$.

$$
\begin{aligned}
&\%RA = \frac{100\%}{n} \sum_{i=1}^{n}\left(1 - \bigg| \frac{x_{i,true}-x_{i,lin}}{x_{i,true}} \bigg|\right)
\\
\\
&\text{where}
\\
&x_{i,true} \ \text{is the true x-axis value.}
\\ 
&x_{i,lin} \ \text{is the linear model predicted x-axis value.}
\\ 
&n \ \text{is the number of data points.}
\end{aligned}
$$ {#eq-pra}

The software also calculates the concavity of the fitted quadratic model to identify if the curve is dominantly non-linear at high (concavity $<0$) or low (concavity $>0$) concentrations.

Supplementary Figure 2 gives the summary statistics of three manually generated curves: A linear curve and two curves with a plateau at high concentrations (denoted as saturation regime curve) and low concentrations (denoted as noise regime curve) respectively. The corresponding Pearson Correlation Coefficient values (`r_corr`) are $\ge0.9$ ($0.99$, $0.95$ and $0.98$ respectively), even for the non-linear curves. However, both non-linear curves give a much lower Mandel's Fitting Test $p$ values (`mandel_p_val`) ($1.66 * 10^{-4}$ and $2.56 * 10^{-3}$ respectively vs $0.38$). Likewise, the Percent Residual Accuracy values (`pra_linear`) are much lower in the non-linear curves compared to the linear curve ($62.30$ and $74.69$ respectively vs $94.32$).

{{< pagebreak >}}

::: {#fig-overview layout="[[60,-40], [65,-35]]"}
![Curve Grouping Workflow](images/README-LinearEvaluation.png){#fig-linearity}

![Interactive Visualisation](images/README-TrellisOutput.png){#fig-visualisation}

*lancer*'s curve grouping workflow in @fig-linearity and interactive visualisation of curves in @fig-visualisation.
:::

{{< pagebreak >}}

After calculating the summary statistics for each curve, *lancer* uses the function `evaluate_linearity` to group the curves according to the workflows proposed in @fig-linearity. Workflow 1 uses the Pearson Correlation Coefficient and Percent Residual Accuracy to determine if the curve is linear (labelled as Good Linearity) or not (labelled as Poor Linearity). Workflow 2 goes one step further, using the Mandel's Fitting Test and the fitted quadratic model's concavity to check if the non-linear curve plateaus at low (labeled as limit of detection) or high (labelled as saturation) concentrations. Non-linear curves that do not follow these trends are labelled as Poor Linearity.

```{r Load Simulated Data Results}
#| echo: false
saturation_regime_curve_summary <-
  readRDS(file = here::here(
    "Quarto_Simulation",
    "saturation_regime_curve_summary.rds")
)

noise_regime_curve_summary <-
  readRDS(file = here::here(
    "Quarto_Simulation",
    "noise_regime_curve_summary.rds")
)

linear_curve_summary <-
  readRDS(file = here::here(
    "Quarto_Simulation",
    "linear_curve_summary.rds")
)
```

```{r Combine Simulated Data Results}
#| echo: false
saturation_regime_curve_summary <- saturation_regime_curve_summary |> 
  dplyr::mutate(curve_group = "Saturation Regime")

noise_regime_curve_summary <- noise_regime_curve_summary |> 
  dplyr::mutate(curve_group = "Noise Regime")

linear_curve_summary <- linear_curve_summary |> 
  dplyr::mutate(curve_group = "Good Linearity")

combined_curve_summary <-
  dplyr::bind_rows(
    saturation_regime_curve_summary,
    noise_regime_curve_summary,
    linear_curve_summary
  ) 
```

```{r Data Wrangling for Simulation Results}
#| echo: false
combined_curve_summary <- 
  combined_curve_summary |> 
  dplyr::mutate(
    curve_group = factor(.data$curve_group,
                         levels = c("Good Linearity",
                                    "Saturation Regime",
                                    "Noise Regime")),
    pearson_group = dplyr::case_when(
      .data$r_corr >= 0.8 ~ "more than or\nequal to 0.8",
      .data$r_corr < 0.8 ~ "less than 0.8",
    ),
    wf2_group = factor(.data$wf2_group,
                       levels = c("Good Linearity",
                                  "Saturation Regime",
                                  "Noise Regime",
                                  "Poor Linearity"))
  ) |> 
  labelled::set_variable_labels(
    ID = "Unique Curve Identifier",
    curve_group = "Simulated Curve Group",
    wf1_group = "Workflow 1 Grouping",
    wf2_group = "Workflow 2 Grouping",
    pearson_group = "Pearson Correlation\nCoefficient Grouping",
    r_corr = "Pearson Correlation\nCoefficient",
    pra_linear = "Percent Residual\nAccuracy",
    mandel_p_val = "Mandel's Test p Value",
    concavity = "Concavity Of Fitted Quadratic Model",
    r2_linear = "Coefficient Of Determination",
    r2_adj_linear = "Adjusted Coefficient Of Determination",
    mandel_stats = "Test statistics from Mandel's Test"
  )

```

```{r Simulation Results Table}
#| echo: false
#| output: false
simulation_results <- combined_curve_summary |> 
  dplyr::select(c("curve_group", "wf2_group", 
                  "pearson_group"
                  # "r_corr", "pra_linear"
                  )
                ) |> 
  dplyr::relocate(c("pearson_group")) |> 
  gtsummary::tbl_summary(
    by = "curve_group",
    statistic = list(
      pearson_group ~ "{n}/{N} ({p}%)",
      wf2_group ~ "{n}/{N} ({p}%)"
    ),
    digits = list(
      wf2_group ~ c(0, 0, 1),
      pearson_group ~ c(0, 0, 1)
      #r_corr ~ c(2),
      #pra_linear ~ c(0)
      )
  ) |> 
  gtsummary::modify_header(
    gtsummary::all_stat_cols() ~ "**{level}**\nN = {n}"
    ) |> 
  gtsummary::modify_spanning_header(
    gtsummary::all_stat_cols() ~ "**Simulated Curve Type**"
    )
```

A benchmark workflow using only Pearson Correlation Coefficient value of $0.8$ is compared with Workflow 2 on simulated data sets of `r nrow(linear_curve_summary)` linear curves (labelled as Linear), curves that plateau at low (labelled as Noise Regime) or high (labelled as Saturation Regime) concentrations. Supplementary Figure 3 shows that Workflow 2 better identifies the saturation and noise regime curves than the benchmark workflow. While Workflow 2 correctly classifies less linear curves than the benchmark workflow, its percentage of correctly classified linear curves `r inline_text(simulation_results, variable = wf2_group, level = "Good Linearity", column = "Good Linearity")` is high. See <https://lancer-simulation.netlify.app> for report details. While the threshold values of Pearson Correlation Coefficient and Percent Residual Accuracy are based on the interpretation of @y.h.chan2003 and @logue2018, respectively, they remain subjective and arbitrary. Nevertheless, *lancer* allows optimization of these threshold values according to the analyst's preference.

Although *lancer* can export the results in Excel or pdf, they may be too complex for meaningful interpretation. @fig-visualisation shows a HTML folder, exported by *lancer*, such that clicking on the `index.html` file inside the folder will open an interactive trellis plot that can group, filter and sort curves. This allows room for exploratory data analysis, such as identifying curves with linearity issues or understanding the effects of changing the Pearson Correlation Coefficient threshold to another value. Such information is hard to achieve with the Excel and pdf files. An example of an interactive viewer created by *lancer* can be viewed at <https://lancer-interactive-example.netlify.app>

# Conclusion

To check if an instrument or experimental method is reliable, it is crucial to check for linearity. However, there are few software that can do this efficiently at a high throughput setting. R package, *lancer*, rectifies this with functions that plots
many curves quickly and reports alternative curve summary statistics, other than the Pearson Correlation Coefficient, to better describe the shape of curves. It also provides an interactive trellis plot for exploratory data analysis. It is available on GitHub <https://github.com/SLINGhub/lancer> while the documentation and tutorials can be accessed from <https://slinghub.github.io/lancer>.

# Acknowledgements

These should be included at the end of the text and not in footnotes. Please ensure you acknowledge all sources of funding, see funding section below.

Details of all funding sources for the work in question should be given in a separate section entitled 'Funding'. This should appear before the 'Acknowledgements' section.

# Funding

The following rules should be followed:

-   The sentence should begin: 'This work was supported by ...' -
-   The full official funding agency name should be given, i.e. 'National Institutes of Health', not 'NIH' (full RIN-approved list of UK funding agencies)
-   Grant numbers should be given in brackets as follows: '\[grant number xxxx\]'
-   Multiple grant numbers should be separated by a comma as follows: '\[grant numbers xxxx, yyyy\]'
-   Agencies should be separated by a semi-colon (plus 'and' before the last funding agency)
-   Where individuals need to be specified for certain sources of funding the following text should be added after the relevant agency or grant number 'to \[author initials\]'.

An example is given here: 'This work was supported by the National Institutes of Health \[AA123456 to C.S., BB765432 to M.H.\]; and the Alcohol & Education Research Council \[hfygr667789\].'

Oxford Journals will deposit all NIH-funded articles in PubMed Central. See Depositing articles in repositories -- information for authors for details. Authors must ensure that manuscripts are clearly indicated as NIH-funded using the guidelines above.

# References
